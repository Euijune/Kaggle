{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet34_Kfold.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yq2t0nduROER",
        "zP5k6WvrgnTB",
        "GjyF1Tveg0A1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSVZKuMHOkNd",
        "outputId": "3f5e82e2-11e7-488b-cde1-54db07c9e4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "\n",
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "\n",
        "# dataset and transformation\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Scheduler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# display images\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# utils\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# submission\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ],
      "metadata": {
        "id": "eFOjM_GoPIQi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define ResNet model**"
      ],
      "metadata": {
        "id": "Yq2t0nduROER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1   # output 채널을 늘리고싶다면 1보다 큰 값으로\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
        "        )\n",
        "\n",
        "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # projection mapping using 1x1conv, input과 output의 feature map size가 다를 경우 사용\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        # identity mapping\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # projection mapping\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qve0Qom9RSxo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=100, init_weights=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels=64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        x = self.conv3_x(output)\n",
        "        x = self.conv4_x(x)\n",
        "        x = self.conv5_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    # define weight initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def resnet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "# 34 = 1(7x7 conv) + 2 * (3 + 4 + 6 + 3) + 1(fc 1000)\n",
        "def resnet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
        "\n",
        "# 50 = 1(7x7 conv) + 3 * (3 + 4 + 6 + 3) + 1(fc 1000)\n",
        "def resnet50():\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=num_classes)\n",
        "\n",
        "def resnet101():\n",
        "    return ResNet(BottleNeck, [3, 4, 23, 3], num_classes=num_classes)\n",
        "\n",
        "def resnet152():\n",
        "    return ResNet(BottleNeck, [3, 8, 36, 3], num_classes=num_classes)"
      ],
      "metadata": {
        "id": "hvqBbM3DRTUE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Utils**"
      ],
      "metadata": {
        "id": "zP5k6WvrgnTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    r\"\"\"Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def print(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # _, pred = output.topk(maxk, 1, True, True)\n",
        "        # pred = pred.t()\n",
        "        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n",
        "        _, idx = output.sort(descending=True)\n",
        "        pred = idx[:,:maxk]\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "1Xa4ViILgpKQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cutout: Main Code for Applying Cutout data augmentation**"
      ],
      "metadata": {
        "id": "GjyF1Tveg0A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "n8eGTZuwgzUF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parameter Setting**"
      ],
      "metadata": {
        "id": "lXJGSMQSRWuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'cifar10' # cifar10 or cifar100\n",
        "model = 'ResNet34' # resnet18, resnet50, resnet101, GoogLeNetV1\n",
        "batch_size = 64  # Input batch size for training (default: 128)\n",
        "epochs = 5 # Number of epochs to train (default: 200)\n",
        "learning_rate = 0.001 # Learning rate\n",
        "data_augmentation = True # Traditional data augmentation such as augmantation by flipping and cropping?\n",
        "fold_num = 5 # k-fold validation\n",
        "path2submission = '/content/drive/Shareddrives/Data/Kaggle/GEK6189_CIFAR-10_Competition_2022-1/submission/'+model+'.csv'    # route for submission .csv file\n",
        "path2weights = f'/content/drive/Shareddrives/Data/Kaggle/GEK6189_CIFAR-10_Competition_2022-1/models/{model}'\n",
        "\n",
        "cutout = False # Apply Cutout?\n",
        "if cutout:\n",
        "    n_holes = 1 # Number of holes to cut out from image\n",
        "    length = 16 # Length of the holes\n",
        "\n",
        "seed = 0 # Random seed (default: 0)\n",
        "print_freq = 100\n",
        "cuda = torch.cuda.is_available()\n",
        "cudnn.benchmark = True  # Should make training should go faster for large models\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "test_id = dataset + '_' + model"
      ],
      "metadata": {
        "id": "ma17ymk8QGUN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Preprocessing\n",
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "# train\n",
        "train_transform = transforms.Compose([])\n",
        "\n",
        "train_transform.transforms.append(transforms.Resize((64, 64)))\n",
        "if data_augmentation:\n",
        "    #train_transform.transforms.append(transforms.RandomCrop(299, 299))\n",
        "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
        "train_transform.transforms.append(transforms.ToTensor())\n",
        "train_transform.transforms.append(normalize)\n",
        "\n",
        "if cutout:\n",
        "    train_transform.transforms.append(Cutout(n_holes=n_holes, length=length))\n",
        "\n",
        "# test\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "\n",
        "# load dataset\n",
        "if dataset == 'cifar10':\n",
        "    num_classes = 10\n",
        "    train_dataset = datasets.CIFAR10(root='data/',\n",
        "                                     train=True,\n",
        "                                     transform=train_transform,\n",
        "                                     download=True)\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(root='data/',\n",
        "                                    train=False,\n",
        "                                    transform=test_transform,\n",
        "                                    download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVTMRhb5PcH0",
        "outputId": "dd2fad6f-8027-4b04-fa1a-99e622b8119c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main Training**"
      ],
      "metadata": {
        "id": "fO4sCjR0i6f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, epoch, model, optimizer, criterion):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(len(train_loader), batch_time, losses,\n",
        "                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "        #input = input.to(torch.device(\"cpu\"))\n",
        "        #target = target.to(torch.device(\"cpu\"))\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss, accuracy \n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0].item(), input.size(0))\n",
        "        top5.update(acc5[0].item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            progress.print(i)\n",
        "\n",
        "    print('==> Train Accuracy: Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "    return top1.avg\n",
        "\n",
        "def valid(val_loader, model):\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    model.eval()\n",
        "    for i,(input,target) in enumerate(val_loader):\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "        #input = input.to(torch.device(\"cpu\"))\n",
        "        #target = target.to(torch.device(\"cpu\"))\n",
        "\n",
        "        output = model(input)\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0].item(), input.size(0))\n",
        "        top5.update(acc5[0].item(), input.size(0))\n",
        "    print('==> Valid Accuracy:  Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "    return top1.avg\n",
        "\n",
        "def test(test_loader, model):  \n",
        "    output = None\n",
        "    \n",
        "    model.eval()\n",
        "    for i, (input, target) in enumerate(tqdm(test_loader), 0):\n",
        "        input = input.cuda()\n",
        "        #target은 쓰지 않음.\n",
        "        #target = target.cuda()\n",
        "\n",
        "        if output is not None:\n",
        "            output = torch.cat((output, model(input)), dim=0)\n",
        "        else:\n",
        "            output = model(input)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "Ky64JtDki8dd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=fold_num, shuffle=True)"
      ],
      "metadata": {
        "id": "C-i8yJnaX60r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation using K-fold cross-validation\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "\n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Ramdom sample elements from a given list of ids, not replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         sampler = train_subsampler,\n",
        "                         pin_memory=True,\n",
        "                         num_workers=2)\n",
        "    \n",
        "    valid_loader = DataLoader(dataset = train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         sampler = valid_subsampler,\n",
        "                         pin_memory=True,\n",
        "                         num_workers=2)\n",
        "    \n",
        "    # Setting\n",
        "    model = resnet34().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='sum').cuda()\n",
        "\n",
        "    # Training\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        # Train for one epoch\n",
        "        train(train_loader, epoch, model, optimizer, criterion)\n",
        "\n",
        "        # Validation\n",
        "        with torch.no_grad():\n",
        "            val_acc = valid(valid_loader, model)\n",
        "\n",
        "        # learning rate scheduling\n",
        "        scheduler.step()\n",
        "    \n",
        "        # Save model for best accuracy\n",
        "        if best_acc < val_acc:\n",
        "            path2weights_fold = f'{path2weights}_fold{fold+1}.pth'  # route for model saving\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), path2weights_fold)\n",
        "\n",
        "    print(f\"Best Top-1 Accuracy for fold{fold+1}: {best_acc}\")"
      ],
      "metadata": {
        "id": "2ZIdY89oYH7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82302e8d-963f-44b9-e507-515c2289aaff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "Epoch: [0][  0/625]\tTime  0.844 ( 0.844)\tLoss 1.4911e+02 (1.4911e+02)\tAcc@1   7.81 (  7.81)\tAcc@5  51.56 ( 51.56)\n",
            "Epoch: [0][100/625]\tTime  0.049 ( 0.057)\tLoss 1.2002e+02 (1.1913e+02)\tAcc@1  32.81 ( 30.40)\tAcc@5  85.94 ( 82.84)\n",
            "Epoch: [0][200/625]\tTime  0.058 ( 0.053)\tLoss 9.1065e+01 (1.1213e+02)\tAcc@1  40.62 ( 34.64)\tAcc@5  95.31 ( 86.25)\n",
            "Epoch: [0][300/625]\tTime  0.047 ( 0.052)\tLoss 9.3589e+01 (1.0731e+02)\tAcc@1  43.75 ( 37.56)\tAcc@5  93.75 ( 87.85)\n",
            "Epoch: [0][400/625]\tTime  0.049 ( 0.051)\tLoss 8.2945e+01 (1.0274e+02)\tAcc@1  53.12 ( 40.42)\tAcc@5  93.75 ( 89.10)\n",
            "Epoch: [0][500/625]\tTime  0.049 ( 0.051)\tLoss 8.9090e+01 (9.9600e+01)\tAcc@1  54.69 ( 42.53)\tAcc@5  90.62 ( 89.95)\n",
            "Epoch: [0][600/625]\tTime  0.048 ( 0.051)\tLoss 7.0642e+01 (9.6440e+01)\tAcc@1  54.69 ( 44.69)\tAcc@5  98.44 ( 90.59)\n",
            "==> Train Accuracy: Acc@1 45.042 || Acc@5 90.780\n",
            "==> Valid Accuracy:  Acc@1 49.790 || Acc@5 92.680\n",
            "Epoch: [1][  0/625]\tTime  0.264 ( 0.264)\tLoss 7.2793e+01 (7.2793e+01)\tAcc@1  60.94 ( 60.94)\tAcc@5  93.75 ( 93.75)\n",
            "Epoch: [1][100/625]\tTime  0.050 ( 0.052)\tLoss 7.2175e+01 (7.7328e+01)\tAcc@1  56.25 ( 57.83)\tAcc@5  93.75 ( 94.72)\n",
            "Epoch: [1][200/625]\tTime  0.050 ( 0.051)\tLoss 8.1428e+01 (7.5618e+01)\tAcc@1  56.25 ( 58.71)\tAcc@5  96.88 ( 95.01)\n",
            "Epoch: [1][300/625]\tTime  0.049 ( 0.051)\tLoss 7.8902e+01 (7.4610e+01)\tAcc@1  57.81 ( 58.99)\tAcc@5  90.62 ( 95.14)\n",
            "Epoch: [1][400/625]\tTime  0.051 ( 0.051)\tLoss 6.9977e+01 (7.2383e+01)\tAcc@1  60.94 ( 60.35)\tAcc@5  96.88 ( 95.52)\n",
            "Epoch: [1][500/625]\tTime  0.059 ( 0.051)\tLoss 5.1592e+01 (7.0433e+01)\tAcc@1  65.62 ( 61.26)\tAcc@5  93.75 ( 95.76)\n",
            "Epoch: [1][600/625]\tTime  0.049 ( 0.052)\tLoss 5.4310e+01 (6.8808e+01)\tAcc@1  71.88 ( 62.27)\tAcc@5  96.88 ( 95.97)\n",
            "==> Train Accuracy: Acc@1 62.425 || Acc@5 95.995\n",
            "==> Valid Accuracy:  Acc@1 66.890 || Acc@5 97.300\n",
            "Epoch: [2][  0/625]\tTime  0.274 ( 0.274)\tLoss 5.9858e+01 (5.9858e+01)\tAcc@1  65.62 ( 65.62)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [2][100/625]\tTime  0.050 ( 0.052)\tLoss 4.8767e+01 (5.6332e+01)\tAcc@1  68.75 ( 68.66)\tAcc@5 100.00 ( 97.12)\n",
            "Epoch: [2][200/625]\tTime  0.065 ( 0.052)\tLoss 4.8900e+01 (5.5577e+01)\tAcc@1  73.44 ( 69.32)\tAcc@5  95.31 ( 97.34)\n",
            "Epoch: [2][300/625]\tTime  0.049 ( 0.051)\tLoss 6.3827e+01 (5.5372e+01)\tAcc@1  67.19 ( 69.65)\tAcc@5  92.19 ( 97.43)\n",
            "Epoch: [2][400/625]\tTime  0.051 ( 0.051)\tLoss 4.8310e+01 (5.4312e+01)\tAcc@1  68.75 ( 70.37)\tAcc@5  98.44 ( 97.54)\n",
            "Epoch: [2][500/625]\tTime  0.049 ( 0.051)\tLoss 5.2021e+01 (5.3760e+01)\tAcc@1  73.44 ( 70.71)\tAcc@5 100.00 ( 97.56)\n",
            "Epoch: [2][600/625]\tTime  0.047 ( 0.050)\tLoss 6.7135e+01 (5.3311e+01)\tAcc@1  64.06 ( 70.84)\tAcc@5  96.88 ( 97.61)\n",
            "==> Train Accuracy: Acc@1 71.015 || Acc@5 97.608\n",
            "==> Valid Accuracy:  Acc@1 72.730 || Acc@5 97.600\n",
            "Epoch: [3][  0/625]\tTime  0.385 ( 0.385)\tLoss 5.2425e+01 (5.2425e+01)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [3][100/625]\tTime  0.049 ( 0.053)\tLoss 5.6471e+01 (4.5549e+01)\tAcc@1  70.31 ( 75.67)\tAcc@5  98.44 ( 98.28)\n",
            "Epoch: [3][200/625]\tTime  0.064 ( 0.053)\tLoss 3.8382e+01 (4.5935e+01)\tAcc@1  79.69 ( 75.30)\tAcc@5 100.00 ( 98.19)\n",
            "Epoch: [3][300/625]\tTime  0.049 ( 0.052)\tLoss 4.5562e+01 (4.5361e+01)\tAcc@1  71.88 ( 75.61)\tAcc@5 100.00 ( 98.22)\n",
            "Epoch: [3][400/625]\tTime  0.049 ( 0.051)\tLoss 4.7684e+01 (4.5127e+01)\tAcc@1  75.00 ( 75.65)\tAcc@5  98.44 ( 98.28)\n",
            "Epoch: [3][500/625]\tTime  0.049 ( 0.051)\tLoss 4.0879e+01 (4.4686e+01)\tAcc@1  71.88 ( 75.89)\tAcc@5 100.00 ( 98.31)\n",
            "Epoch: [3][600/625]\tTime  0.049 ( 0.051)\tLoss 4.9631e+01 (4.4397e+01)\tAcc@1  73.44 ( 76.06)\tAcc@5  98.44 ( 98.36)\n",
            "==> Train Accuracy: Acc@1 76.065 || Acc@5 98.392\n",
            "==> Valid Accuracy:  Acc@1 75.330 || Acc@5 98.240\n",
            "Epoch: [4][  0/625]\tTime  0.278 ( 0.278)\tLoss 3.2674e+01 (3.2674e+01)\tAcc@1  81.25 ( 81.25)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [4][100/625]\tTime  0.047 ( 0.052)\tLoss 3.1572e+01 (3.7716e+01)\tAcc@1  82.81 ( 79.66)\tAcc@5 100.00 ( 98.78)\n",
            "Epoch: [4][200/625]\tTime  0.084 ( 0.052)\tLoss 4.0445e+01 (3.7991e+01)\tAcc@1  78.12 ( 79.62)\tAcc@5 100.00 ( 98.82)\n",
            "Epoch: [4][300/625]\tTime  0.049 ( 0.051)\tLoss 5.0816e+01 (3.8537e+01)\tAcc@1  75.00 ( 79.46)\tAcc@5  96.88 ( 98.72)\n",
            "Epoch: [4][400/625]\tTime  0.048 ( 0.051)\tLoss 4.0636e+01 (3.8330e+01)\tAcc@1  84.38 ( 79.51)\tAcc@5  95.31 ( 98.71)\n",
            "Epoch: [4][500/625]\tTime  0.049 ( 0.051)\tLoss 2.6621e+01 (3.8015e+01)\tAcc@1  79.69 ( 79.55)\tAcc@5 100.00 ( 98.76)\n",
            "Epoch: [4][600/625]\tTime  0.047 ( 0.050)\tLoss 3.4575e+01 (3.8167e+01)\tAcc@1  81.25 ( 79.47)\tAcc@5 100.00 ( 98.78)\n",
            "==> Train Accuracy: Acc@1 79.493 || Acc@5 98.790\n",
            "==> Valid Accuracy:  Acc@1 77.840 || Acc@5 98.640\n",
            "Best Top-1 Accuracy for fold1: 77.84\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Epoch: [0][  0/625]\tTime  0.354 ( 0.354)\tLoss 1.5062e+02 (1.5062e+02)\tAcc@1   7.81 (  7.81)\tAcc@5  40.62 ( 40.62)\n",
            "Epoch: [0][100/625]\tTime  0.080 ( 0.053)\tLoss 1.1932e+02 (1.2044e+02)\tAcc@1  28.12 ( 29.36)\tAcc@5  79.69 ( 82.16)\n",
            "Epoch: [0][200/625]\tTime  0.048 ( 0.052)\tLoss 9.9846e+01 (1.1278e+02)\tAcc@1  46.88 ( 34.31)\tAcc@5  90.62 ( 85.63)\n",
            "Epoch: [0][300/625]\tTime  0.047 ( 0.051)\tLoss 7.5608e+01 (1.0723e+02)\tAcc@1  45.31 ( 37.71)\tAcc@5 100.00 ( 87.59)\n",
            "Epoch: [0][400/625]\tTime  0.047 ( 0.051)\tLoss 8.9888e+01 (1.0282e+02)\tAcc@1  43.75 ( 40.38)\tAcc@5  90.62 ( 88.93)\n",
            "Epoch: [0][500/625]\tTime  0.054 ( 0.050)\tLoss 7.2588e+01 (9.9528e+01)\tAcc@1  65.62 ( 42.46)\tAcc@5  96.88 ( 89.88)\n",
            "Epoch: [0][600/625]\tTime  0.049 ( 0.050)\tLoss 8.1708e+01 (9.6349e+01)\tAcc@1  56.25 ( 44.52)\tAcc@5  92.19 ( 90.59)\n",
            "==> Train Accuracy: Acc@1 45.050 || Acc@5 90.755\n",
            "==> Valid Accuracy:  Acc@1 54.820 || Acc@5 94.470\n",
            "Epoch: [1][  0/625]\tTime  0.270 ( 0.270)\tLoss 8.1174e+01 (8.1174e+01)\tAcc@1  60.94 ( 60.94)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [1][100/625]\tTime  0.052 ( 0.052)\tLoss 6.4637e+01 (7.4461e+01)\tAcc@1  64.06 ( 58.83)\tAcc@5  95.31 ( 95.59)\n",
            "Epoch: [1][200/625]\tTime  0.056 ( 0.051)\tLoss 6.6464e+01 (7.4049e+01)\tAcc@1  60.94 ( 59.02)\tAcc@5  96.88 ( 95.50)\n",
            "Epoch: [1][300/625]\tTime  0.049 ( 0.051)\tLoss 6.3896e+01 (7.2795e+01)\tAcc@1  62.50 ( 59.86)\tAcc@5  95.31 ( 95.64)\n",
            "Epoch: [1][400/625]\tTime  0.052 ( 0.051)\tLoss 6.5565e+01 (7.1143e+01)\tAcc@1  68.75 ( 60.66)\tAcc@5  92.19 ( 95.87)\n",
            "Epoch: [1][500/625]\tTime  0.047 ( 0.050)\tLoss 5.1073e+01 (6.9663e+01)\tAcc@1  68.75 ( 61.69)\tAcc@5  96.88 ( 96.01)\n",
            "Epoch: [1][600/625]\tTime  0.050 ( 0.050)\tLoss 7.3691e+01 (6.8310e+01)\tAcc@1  65.62 ( 62.56)\tAcc@5  90.62 ( 96.17)\n",
            "==> Train Accuracy: Acc@1 62.672 || Acc@5 96.168\n",
            "==> Valid Accuracy:  Acc@1 64.600 || Acc@5 96.800\n",
            "Epoch: [2][  0/625]\tTime  0.401 ( 0.401)\tLoss 5.6366e+01 (5.6366e+01)\tAcc@1  68.75 ( 68.75)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [2][100/625]\tTime  0.047 ( 0.053)\tLoss 6.7164e+01 (5.7302e+01)\tAcc@1  68.75 ( 69.21)\tAcc@5  98.44 ( 97.43)\n",
            "Epoch: [2][200/625]\tTime  0.063 ( 0.052)\tLoss 3.9128e+01 (5.6394e+01)\tAcc@1  79.69 ( 69.64)\tAcc@5 100.00 ( 97.43)\n",
            "Epoch: [2][300/625]\tTime  0.047 ( 0.051)\tLoss 7.6080e+01 (5.5721e+01)\tAcc@1  60.94 ( 69.95)\tAcc@5  96.88 ( 97.38)\n",
            "Epoch: [2][400/625]\tTime  0.047 ( 0.051)\tLoss 4.6301e+01 (5.5345e+01)\tAcc@1  73.44 ( 70.00)\tAcc@5 100.00 ( 97.48)\n",
            "Epoch: [2][500/625]\tTime  0.050 ( 0.050)\tLoss 6.4143e+01 (5.4514e+01)\tAcc@1  60.94 ( 70.39)\tAcc@5  98.44 ( 97.55)\n",
            "Epoch: [2][600/625]\tTime  0.047 ( 0.050)\tLoss 5.0984e+01 (5.3745e+01)\tAcc@1  71.88 ( 70.83)\tAcc@5 100.00 ( 97.60)\n",
            "==> Train Accuracy: Acc@1 70.832 || Acc@5 97.605\n",
            "==> Valid Accuracy:  Acc@1 71.620 || Acc@5 97.850\n",
            "Epoch: [3][  0/625]\tTime  0.299 ( 0.299)\tLoss 5.8660e+01 (5.8660e+01)\tAcc@1  71.88 ( 71.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [3][100/625]\tTime  0.048 ( 0.052)\tLoss 4.3588e+01 (4.6281e+01)\tAcc@1  76.56 ( 75.34)\tAcc@5 100.00 ( 98.24)\n",
            "Epoch: [3][200/625]\tTime  0.050 ( 0.051)\tLoss 4.4949e+01 (4.5961e+01)\tAcc@1  73.44 ( 75.09)\tAcc@5  96.88 ( 98.18)\n",
            "Epoch: [3][300/625]\tTime  0.048 ( 0.051)\tLoss 4.5922e+01 (4.5702e+01)\tAcc@1  73.44 ( 75.25)\tAcc@5  96.88 ( 98.28)\n",
            "Epoch: [3][400/625]\tTime  0.047 ( 0.050)\tLoss 3.9132e+01 (4.5262e+01)\tAcc@1  79.69 ( 75.38)\tAcc@5 100.00 ( 98.34)\n",
            "Epoch: [3][500/625]\tTime  0.054 ( 0.050)\tLoss 5.4197e+01 (4.5041e+01)\tAcc@1  71.88 ( 75.58)\tAcc@5  98.44 ( 98.36)\n",
            "Epoch: [3][600/625]\tTime  0.049 ( 0.050)\tLoss 6.9387e+01 (4.5114e+01)\tAcc@1  71.88 ( 75.54)\tAcc@5  96.88 ( 98.33)\n",
            "==> Train Accuracy: Acc@1 75.665 || Acc@5 98.340\n",
            "==> Valid Accuracy:  Acc@1 74.590 || Acc@5 98.280\n",
            "Epoch: [4][  0/625]\tTime  0.284 ( 0.284)\tLoss 4.3705e+01 (4.3705e+01)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [4][100/625]\tTime  0.049 ( 0.052)\tLoss 4.7403e+01 (3.8014e+01)\tAcc@1  78.12 ( 79.55)\tAcc@5  98.44 ( 98.70)\n",
            "Epoch: [4][200/625]\tTime  0.067 ( 0.051)\tLoss 4.4882e+01 (3.8153e+01)\tAcc@1  73.44 ( 79.34)\tAcc@5  98.44 ( 98.84)\n",
            "Epoch: [4][300/625]\tTime  0.048 ( 0.051)\tLoss 4.2233e+01 (3.8502e+01)\tAcc@1  79.69 ( 79.11)\tAcc@5 100.00 ( 98.83)\n",
            "Epoch: [4][400/625]\tTime  0.053 ( 0.050)\tLoss 5.0488e+01 (3.8647e+01)\tAcc@1  78.12 ( 79.04)\tAcc@5  96.88 ( 98.82)\n",
            "Epoch: [4][500/625]\tTime  0.047 ( 0.050)\tLoss 4.2602e+01 (3.8569e+01)\tAcc@1  75.00 ( 79.14)\tAcc@5  96.88 ( 98.83)\n",
            "Epoch: [4][600/625]\tTime  0.049 ( 0.050)\tLoss 3.1607e+01 (3.8538e+01)\tAcc@1  81.25 ( 79.20)\tAcc@5  98.44 ( 98.82)\n",
            "==> Train Accuracy: Acc@1 79.285 || Acc@5 98.828\n",
            "==> Valid Accuracy:  Acc@1 77.240 || Acc@5 98.480\n",
            "Best Top-1 Accuracy for fold2: 77.24\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Epoch: [0][  0/625]\tTime  0.356 ( 0.356)\tLoss 1.4741e+02 (1.4741e+02)\tAcc@1  10.94 ( 10.94)\tAcc@5  57.81 ( 57.81)\n",
            "Epoch: [0][100/625]\tTime  0.049 ( 0.052)\tLoss 1.1146e+02 (1.1894e+02)\tAcc@1  35.94 ( 29.66)\tAcc@5  90.62 ( 83.63)\n",
            "Epoch: [0][200/625]\tTime  0.051 ( 0.052)\tLoss 8.9445e+01 (1.1059e+02)\tAcc@1  46.88 ( 34.76)\tAcc@5  92.19 ( 87.11)\n",
            "Epoch: [0][300/625]\tTime  0.048 ( 0.051)\tLoss 8.2124e+01 (1.0593e+02)\tAcc@1  50.00 ( 37.99)\tAcc@5  95.31 ( 88.41)\n",
            "Epoch: [0][400/625]\tTime  0.047 ( 0.050)\tLoss 9.7544e+01 (1.0226e+02)\tAcc@1  46.88 ( 40.37)\tAcc@5  95.31 ( 89.45)\n",
            "Epoch: [0][500/625]\tTime  0.048 ( 0.050)\tLoss 7.7442e+01 (9.8830e+01)\tAcc@1  59.38 ( 42.76)\tAcc@5  93.75 ( 90.24)\n",
            "Epoch: [0][600/625]\tTime  0.047 ( 0.050)\tLoss 8.7610e+01 (9.5969e+01)\tAcc@1  56.25 ( 44.72)\tAcc@5  87.50 ( 90.80)\n",
            "==> Train Accuracy: Acc@1 45.153 || Acc@5 90.912\n",
            "==> Valid Accuracy:  Acc@1 55.290 || Acc@5 95.140\n",
            "Epoch: [1][  0/625]\tTime  0.258 ( 0.258)\tLoss 7.1774e+01 (7.1774e+01)\tAcc@1  57.81 ( 57.81)\tAcc@5  92.19 ( 92.19)\n",
            "Epoch: [1][100/625]\tTime  0.048 ( 0.051)\tLoss 7.8431e+01 (7.6576e+01)\tAcc@1  50.00 ( 57.70)\tAcc@5  93.75 ( 94.59)\n",
            "Epoch: [1][200/625]\tTime  0.049 ( 0.051)\tLoss 7.0556e+01 (7.4182e+01)\tAcc@1  62.50 ( 58.70)\tAcc@5  95.31 ( 95.00)\n",
            "Epoch: [1][300/625]\tTime  0.049 ( 0.051)\tLoss 7.3869e+01 (7.2262e+01)\tAcc@1  59.38 ( 59.95)\tAcc@5  95.31 ( 95.40)\n",
            "Epoch: [1][400/625]\tTime  0.047 ( 0.050)\tLoss 6.7116e+01 (7.1393e+01)\tAcc@1  62.50 ( 60.44)\tAcc@5  95.31 ( 95.54)\n",
            "Epoch: [1][500/625]\tTime  0.050 ( 0.050)\tLoss 5.6687e+01 (6.9940e+01)\tAcc@1  70.31 ( 61.33)\tAcc@5  96.88 ( 95.78)\n",
            "Epoch: [1][600/625]\tTime  0.049 ( 0.050)\tLoss 6.1986e+01 (6.8962e+01)\tAcc@1  64.06 ( 61.86)\tAcc@5 100.00 ( 95.90)\n",
            "==> Train Accuracy: Acc@1 61.940 || Acc@5 95.940\n",
            "==> Valid Accuracy:  Acc@1 62.860 || Acc@5 95.580\n",
            "Epoch: [2][  0/625]\tTime  0.405 ( 0.405)\tLoss 6.6868e+01 (6.6868e+01)\tAcc@1  62.50 ( 62.50)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [2][100/625]\tTime  0.048 ( 0.053)\tLoss 5.2699e+01 (5.6857e+01)\tAcc@1  67.19 ( 69.34)\tAcc@5  98.44 ( 97.49)\n",
            "Epoch: [2][200/625]\tTime  0.056 ( 0.051)\tLoss 4.7074e+01 (5.6392e+01)\tAcc@1  71.88 ( 69.41)\tAcc@5 100.00 ( 97.40)\n",
            "Epoch: [2][300/625]\tTime  0.049 ( 0.051)\tLoss 5.0480e+01 (5.5990e+01)\tAcc@1  78.12 ( 69.52)\tAcc@5  95.31 ( 97.38)\n",
            "Epoch: [2][400/625]\tTime  0.050 ( 0.050)\tLoss 6.1354e+01 (5.5121e+01)\tAcc@1  70.31 ( 69.81)\tAcc@5 100.00 ( 97.48)\n",
            "Epoch: [2][500/625]\tTime  0.045 ( 0.050)\tLoss 5.1073e+01 (5.4407e+01)\tAcc@1  73.44 ( 70.24)\tAcc@5 100.00 ( 97.47)\n",
            "Epoch: [2][600/625]\tTime  0.048 ( 0.050)\tLoss 4.0733e+01 (5.3816e+01)\tAcc@1  79.69 ( 70.66)\tAcc@5 100.00 ( 97.50)\n",
            "==> Train Accuracy: Acc@1 70.805 || Acc@5 97.520\n",
            "==> Valid Accuracy:  Acc@1 70.540 || Acc@5 97.720\n",
            "Epoch: [3][  0/625]\tTime  0.294 ( 0.294)\tLoss 4.4096e+01 (4.4096e+01)\tAcc@1  79.69 ( 79.69)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [3][100/625]\tTime  0.047 ( 0.052)\tLoss 5.3108e+01 (4.6704e+01)\tAcc@1  75.00 ( 75.06)\tAcc@5  96.88 ( 98.17)\n",
            "Epoch: [3][200/625]\tTime  0.066 ( 0.051)\tLoss 5.8338e+01 (4.7373e+01)\tAcc@1  67.19 ( 74.37)\tAcc@5  96.88 ( 98.19)\n",
            "Epoch: [3][300/625]\tTime  0.050 ( 0.051)\tLoss 5.3776e+01 (4.6030e+01)\tAcc@1  70.31 ( 75.22)\tAcc@5  96.88 ( 98.29)\n",
            "Epoch: [3][400/625]\tTime  0.049 ( 0.050)\tLoss 5.8886e+01 (4.5818e+01)\tAcc@1  70.31 ( 75.42)\tAcc@5  96.88 ( 98.32)\n",
            "Epoch: [3][500/625]\tTime  0.048 ( 0.050)\tLoss 2.8963e+01 (4.5809e+01)\tAcc@1  82.81 ( 75.35)\tAcc@5 100.00 ( 98.34)\n",
            "Epoch: [3][600/625]\tTime  0.058 ( 0.050)\tLoss 5.0212e+01 (4.5193e+01)\tAcc@1  70.31 ( 75.64)\tAcc@5 100.00 ( 98.38)\n",
            "==> Train Accuracy: Acc@1 75.722 || Acc@5 98.385\n",
            "==> Valid Accuracy:  Acc@1 74.820 || Acc@5 98.440\n",
            "Epoch: [4][  0/625]\tTime  0.363 ( 0.363)\tLoss 3.3347e+01 (3.3347e+01)\tAcc@1  79.69 ( 79.69)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [4][100/625]\tTime  0.056 ( 0.052)\tLoss 3.4570e+01 (3.8014e+01)\tAcc@1  82.81 ( 79.08)\tAcc@5 100.00 ( 98.95)\n",
            "Epoch: [4][200/625]\tTime  0.052 ( 0.051)\tLoss 3.6969e+01 (3.8582e+01)\tAcc@1  78.12 ( 78.91)\tAcc@5  95.31 ( 98.74)\n",
            "Epoch: [4][300/625]\tTime  0.049 ( 0.051)\tLoss 3.1673e+01 (3.8864e+01)\tAcc@1  84.38 ( 78.66)\tAcc@5  98.44 ( 98.72)\n",
            "Epoch: [4][400/625]\tTime  0.049 ( 0.050)\tLoss 2.7361e+01 (3.8758e+01)\tAcc@1  84.38 ( 78.72)\tAcc@5  98.44 ( 98.72)\n",
            "Epoch: [4][500/625]\tTime  0.053 ( 0.050)\tLoss 3.5501e+01 (3.8514e+01)\tAcc@1  81.25 ( 78.88)\tAcc@5 100.00 ( 98.73)\n",
            "Epoch: [4][600/625]\tTime  0.047 ( 0.050)\tLoss 3.0180e+01 (3.8437e+01)\tAcc@1  84.38 ( 79.05)\tAcc@5 100.00 ( 98.75)\n",
            "==> Train Accuracy: Acc@1 79.055 || Acc@5 98.755\n",
            "==> Valid Accuracy:  Acc@1 78.550 || Acc@5 98.690\n",
            "Best Top-1 Accuracy for fold3: 78.55\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Epoch: [0][  0/625]\tTime  0.247 ( 0.247)\tLoss 1.4909e+02 (1.4909e+02)\tAcc@1   6.25 (  6.25)\tAcc@5  56.25 ( 56.25)\n",
            "Epoch: [0][100/625]\tTime  0.052 ( 0.052)\tLoss 1.1304e+02 (1.1867e+02)\tAcc@1  31.25 ( 29.90)\tAcc@5  85.94 ( 83.09)\n",
            "Epoch: [0][200/625]\tTime  0.050 ( 0.051)\tLoss 1.0869e+02 (1.1113e+02)\tAcc@1  35.94 ( 34.72)\tAcc@5  89.06 ( 86.39)\n",
            "Epoch: [0][300/625]\tTime  0.049 ( 0.051)\tLoss 9.4380e+01 (1.0628e+02)\tAcc@1  48.44 ( 38.11)\tAcc@5  90.62 ( 88.10)\n",
            "Epoch: [0][400/625]\tTime  0.049 ( 0.050)\tLoss 1.1074e+02 (1.0260e+02)\tAcc@1  37.50 ( 40.54)\tAcc@5  87.50 ( 89.20)\n",
            "Epoch: [0][500/625]\tTime  0.050 ( 0.050)\tLoss 8.7261e+01 (9.9341e+01)\tAcc@1  48.44 ( 42.57)\tAcc@5  93.75 ( 90.06)\n",
            "Epoch: [0][600/625]\tTime  0.048 ( 0.050)\tLoss 8.1838e+01 (9.6370e+01)\tAcc@1  50.00 ( 44.56)\tAcc@5  93.75 ( 90.72)\n",
            "==> Train Accuracy: Acc@1 44.940 || Acc@5 90.903\n",
            "==> Valid Accuracy:  Acc@1 54.470 || Acc@5 94.450\n",
            "Epoch: [1][  0/625]\tTime  0.337 ( 0.337)\tLoss 8.1244e+01 (8.1244e+01)\tAcc@1  53.12 ( 53.12)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [1][100/625]\tTime  0.054 ( 0.053)\tLoss 8.3153e+01 (7.6516e+01)\tAcc@1  53.12 ( 57.07)\tAcc@5  95.31 ( 95.14)\n",
            "Epoch: [1][200/625]\tTime  0.049 ( 0.051)\tLoss 6.2080e+01 (7.4691e+01)\tAcc@1  68.75 ( 58.19)\tAcc@5  95.31 ( 95.30)\n",
            "Epoch: [1][300/625]\tTime  0.048 ( 0.051)\tLoss 6.4007e+01 (7.2844e+01)\tAcc@1  68.75 ( 59.56)\tAcc@5  95.31 ( 95.58)\n",
            "Epoch: [1][400/625]\tTime  0.053 ( 0.051)\tLoss 6.3117e+01 (7.1424e+01)\tAcc@1  64.06 ( 60.44)\tAcc@5  93.75 ( 95.76)\n",
            "Epoch: [1][500/625]\tTime  0.050 ( 0.050)\tLoss 5.8382e+01 (6.9713e+01)\tAcc@1  70.31 ( 61.45)\tAcc@5  92.19 ( 95.94)\n",
            "Epoch: [1][600/625]\tTime  0.048 ( 0.050)\tLoss 4.9206e+01 (6.8276e+01)\tAcc@1  73.44 ( 62.31)\tAcc@5  96.88 ( 96.07)\n",
            "==> Train Accuracy: Acc@1 62.515 || Acc@5 96.100\n",
            "==> Valid Accuracy:  Acc@1 67.860 || Acc@5 97.230\n",
            "Epoch: [2][  0/625]\tTime  0.301 ( 0.301)\tLoss 6.3823e+01 (6.3823e+01)\tAcc@1  60.94 ( 60.94)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [2][100/625]\tTime  0.047 ( 0.052)\tLoss 7.3939e+01 (5.5739e+01)\tAcc@1  60.94 ( 70.10)\tAcc@5  96.88 ( 97.42)\n",
            "Epoch: [2][200/625]\tTime  0.061 ( 0.051)\tLoss 4.0637e+01 (5.5357e+01)\tAcc@1  78.12 ( 69.89)\tAcc@5  96.88 ( 97.41)\n",
            "Epoch: [2][300/625]\tTime  0.048 ( 0.051)\tLoss 5.1616e+01 (5.5559e+01)\tAcc@1  73.44 ( 69.81)\tAcc@5 100.00 ( 97.43)\n",
            "Epoch: [2][400/625]\tTime  0.048 ( 0.050)\tLoss 4.6204e+01 (5.4509e+01)\tAcc@1  78.12 ( 70.44)\tAcc@5  98.44 ( 97.60)\n",
            "Epoch: [2][500/625]\tTime  0.050 ( 0.050)\tLoss 3.9708e+01 (5.4223e+01)\tAcc@1  78.12 ( 70.57)\tAcc@5  98.44 ( 97.66)\n",
            "Epoch: [2][600/625]\tTime  0.053 ( 0.050)\tLoss 4.9516e+01 (5.3498e+01)\tAcc@1  75.00 ( 71.00)\tAcc@5 100.00 ( 97.72)\n",
            "==> Train Accuracy: Acc@1 71.020 || Acc@5 97.728\n",
            "==> Valid Accuracy:  Acc@1 71.430 || Acc@5 97.600\n",
            "Epoch: [3][  0/625]\tTime  0.296 ( 0.296)\tLoss 4.7073e+01 (4.7073e+01)\tAcc@1  70.31 ( 70.31)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [3][100/625]\tTime  0.046 ( 0.052)\tLoss 3.5726e+01 (4.2998e+01)\tAcc@1  81.25 ( 77.09)\tAcc@5 100.00 ( 98.65)\n",
            "Epoch: [3][200/625]\tTime  0.060 ( 0.051)\tLoss 4.0229e+01 (4.4175e+01)\tAcc@1  76.56 ( 76.41)\tAcc@5 100.00 ( 98.42)\n",
            "Epoch: [3][300/625]\tTime  0.048 ( 0.051)\tLoss 4.8093e+01 (4.4348e+01)\tAcc@1  71.88 ( 76.38)\tAcc@5  98.44 ( 98.39)\n",
            "Epoch: [3][400/625]\tTime  0.049 ( 0.050)\tLoss 5.3011e+01 (4.3766e+01)\tAcc@1  76.56 ( 76.67)\tAcc@5  95.31 ( 98.46)\n",
            "Epoch: [3][500/625]\tTime  0.052 ( 0.050)\tLoss 5.0954e+01 (4.3988e+01)\tAcc@1  73.44 ( 76.57)\tAcc@5  96.88 ( 98.41)\n",
            "Epoch: [3][600/625]\tTime  0.049 ( 0.050)\tLoss 3.9928e+01 (4.3732e+01)\tAcc@1  81.25 ( 76.61)\tAcc@5 100.00 ( 98.44)\n",
            "==> Train Accuracy: Acc@1 76.685 || Acc@5 98.435\n",
            "==> Valid Accuracy:  Acc@1 76.510 || Acc@5 98.150\n",
            "Epoch: [4][  0/625]\tTime  0.383 ( 0.383)\tLoss 2.7908e+01 (2.7908e+01)\tAcc@1  81.25 ( 81.25)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [4][100/625]\tTime  0.048 ( 0.053)\tLoss 4.3218e+01 (3.6054e+01)\tAcc@1  79.69 ( 80.68)\tAcc@5  96.88 ( 99.03)\n",
            "Epoch: [4][200/625]\tTime  0.064 ( 0.052)\tLoss 6.4619e+01 (3.7698e+01)\tAcc@1  75.00 ( 80.02)\tAcc@5  93.75 ( 98.84)\n",
            "Epoch: [4][300/625]\tTime  0.048 ( 0.051)\tLoss 4.5078e+01 (3.7560e+01)\tAcc@1  70.31 ( 80.04)\tAcc@5 100.00 ( 98.89)\n",
            "Epoch: [4][400/625]\tTime  0.060 ( 0.051)\tLoss 4.3518e+01 (3.7534e+01)\tAcc@1  76.56 ( 79.97)\tAcc@5  98.44 ( 98.85)\n",
            "Epoch: [4][500/625]\tTime  0.054 ( 0.050)\tLoss 3.7362e+01 (3.7346e+01)\tAcc@1  79.69 ( 80.01)\tAcc@5 100.00 ( 98.85)\n",
            "Epoch: [4][600/625]\tTime  0.048 ( 0.050)\tLoss 3.4217e+01 (3.7452e+01)\tAcc@1  84.38 ( 79.92)\tAcc@5 100.00 ( 98.81)\n",
            "==> Train Accuracy: Acc@1 79.933 || Acc@5 98.820\n",
            "==> Valid Accuracy:  Acc@1 77.980 || Acc@5 98.450\n",
            "Best Top-1 Accuracy for fold4: 77.98\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Epoch: [0][  0/625]\tTime  0.255 ( 0.255)\tLoss 1.4801e+02 (1.4801e+02)\tAcc@1  12.50 ( 12.50)\tAcc@5  53.12 ( 53.12)\n",
            "Epoch: [0][100/625]\tTime  0.048 ( 0.051)\tLoss 1.1665e+02 (1.1914e+02)\tAcc@1  29.69 ( 29.46)\tAcc@5  87.50 ( 82.92)\n",
            "Epoch: [0][200/625]\tTime  0.060 ( 0.051)\tLoss 1.0985e+02 (1.1126e+02)\tAcc@1  39.06 ( 34.72)\tAcc@5  87.50 ( 86.16)\n",
            "Epoch: [0][300/625]\tTime  0.051 ( 0.051)\tLoss 9.2001e+01 (1.0544e+02)\tAcc@1  46.88 ( 38.69)\tAcc@5  93.75 ( 88.03)\n",
            "Epoch: [0][400/625]\tTime  0.049 ( 0.050)\tLoss 9.4999e+01 (1.0117e+02)\tAcc@1  43.75 ( 41.56)\tAcc@5  93.75 ( 89.17)\n",
            "Epoch: [0][500/625]\tTime  0.047 ( 0.050)\tLoss 8.2121e+01 (9.7434e+01)\tAcc@1  57.81 ( 44.00)\tAcc@5  92.19 ( 90.05)\n",
            "Epoch: [0][600/625]\tTime  0.046 ( 0.050)\tLoss 6.6091e+01 (9.4506e+01)\tAcc@1  64.06 ( 45.86)\tAcc@5  95.31 ( 90.73)\n",
            "==> Train Accuracy: Acc@1 46.267 || Acc@5 90.865\n",
            "==> Valid Accuracy:  Acc@1 57.300 || Acc@5 94.770\n",
            "Epoch: [1][  0/625]\tTime  0.362 ( 0.362)\tLoss 6.8428e+01 (6.8428e+01)\tAcc@1  60.94 ( 60.94)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [1][100/625]\tTime  0.048 ( 0.052)\tLoss 8.1852e+01 (7.3296e+01)\tAcc@1  67.19 ( 59.79)\tAcc@5  93.75 ( 94.69)\n",
            "Epoch: [1][200/625]\tTime  0.073 ( 0.052)\tLoss 9.2138e+01 (7.1852e+01)\tAcc@1  51.56 ( 60.28)\tAcc@5  93.75 ( 95.27)\n",
            "Epoch: [1][300/625]\tTime  0.048 ( 0.051)\tLoss 6.7999e+01 (7.0827e+01)\tAcc@1  53.12 ( 61.00)\tAcc@5  96.88 ( 95.48)\n",
            "Epoch: [1][400/625]\tTime  0.051 ( 0.050)\tLoss 4.5957e+01 (6.9299e+01)\tAcc@1  73.44 ( 61.86)\tAcc@5 100.00 ( 95.80)\n",
            "Epoch: [1][500/625]\tTime  0.047 ( 0.050)\tLoss 5.2553e+01 (6.8106e+01)\tAcc@1  71.88 ( 62.50)\tAcc@5  98.44 ( 96.01)\n",
            "Epoch: [1][600/625]\tTime  0.047 ( 0.050)\tLoss 7.8122e+01 (6.7074e+01)\tAcc@1  59.38 ( 63.01)\tAcc@5  95.31 ( 96.12)\n",
            "==> Train Accuracy: Acc@1 63.157 || Acc@5 96.150\n",
            "==> Valid Accuracy:  Acc@1 62.730 || Acc@5 96.310\n",
            "Epoch: [2][  0/625]\tTime  0.287 ( 0.287)\tLoss 6.9560e+01 (6.9560e+01)\tAcc@1  64.06 ( 64.06)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [2][100/625]\tTime  0.048 ( 0.052)\tLoss 7.0969e+01 (5.6863e+01)\tAcc@1  60.94 ( 69.37)\tAcc@5  93.75 ( 97.20)\n",
            "Epoch: [2][200/625]\tTime  0.054 ( 0.051)\tLoss 5.1188e+01 (5.5017e+01)\tAcc@1  71.88 ( 70.09)\tAcc@5  98.44 ( 97.48)\n",
            "Epoch: [2][300/625]\tTime  0.047 ( 0.051)\tLoss 4.9183e+01 (5.4871e+01)\tAcc@1  68.75 ( 70.10)\tAcc@5  98.44 ( 97.51)\n",
            "Epoch: [2][400/625]\tTime  0.047 ( 0.050)\tLoss 4.5152e+01 (5.4009e+01)\tAcc@1  73.44 ( 70.48)\tAcc@5  98.44 ( 97.56)\n",
            "Epoch: [2][500/625]\tTime  0.048 ( 0.050)\tLoss 6.3588e+01 (5.3571e+01)\tAcc@1  62.50 ( 70.69)\tAcc@5 100.00 ( 97.60)\n",
            "Epoch: [2][600/625]\tTime  0.050 ( 0.050)\tLoss 6.6076e+01 (5.3385e+01)\tAcc@1  59.38 ( 70.81)\tAcc@5 100.00 ( 97.65)\n",
            "==> Train Accuracy: Acc@1 70.888 || Acc@5 97.677\n",
            "==> Valid Accuracy:  Acc@1 70.500 || Acc@5 97.630\n",
            "Epoch: [3][  0/625]\tTime  0.377 ( 0.377)\tLoss 5.1633e+01 (5.1633e+01)\tAcc@1  70.31 ( 70.31)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [3][100/625]\tTime  0.047 ( 0.053)\tLoss 5.2575e+01 (4.5111e+01)\tAcc@1  73.44 ( 75.79)\tAcc@5  98.44 ( 98.28)\n",
            "Epoch: [3][200/625]\tTime  0.050 ( 0.051)\tLoss 5.0785e+01 (4.5164e+01)\tAcc@1  68.75 ( 75.38)\tAcc@5  96.88 ( 98.27)\n",
            "Epoch: [3][300/625]\tTime  0.047 ( 0.051)\tLoss 3.2393e+01 (4.5400e+01)\tAcc@1  82.81 ( 75.21)\tAcc@5 100.00 ( 98.29)\n",
            "Epoch: [3][400/625]\tTime  0.047 ( 0.050)\tLoss 4.5593e+01 (4.4941e+01)\tAcc@1  76.56 ( 75.62)\tAcc@5  98.44 ( 98.27)\n",
            "Epoch: [3][500/625]\tTime  0.049 ( 0.050)\tLoss 3.6998e+01 (4.4710e+01)\tAcc@1  82.81 ( 75.68)\tAcc@5 100.00 ( 98.31)\n",
            "Epoch: [3][600/625]\tTime  0.052 ( 0.050)\tLoss 5.7272e+01 (4.3967e+01)\tAcc@1  68.75 ( 76.10)\tAcc@5  96.88 ( 98.34)\n",
            "==> Train Accuracy: Acc@1 76.125 || Acc@5 98.347\n",
            "==> Valid Accuracy:  Acc@1 73.860 || Acc@5 97.870\n",
            "Epoch: [4][  0/625]\tTime  0.269 ( 0.269)\tLoss 3.8229e+01 (3.8229e+01)\tAcc@1  78.12 ( 78.12)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [4][100/625]\tTime  0.052 ( 0.052)\tLoss 5.0588e+01 (3.9546e+01)\tAcc@1  76.56 ( 78.81)\tAcc@5 100.00 ( 98.53)\n",
            "Epoch: [4][200/625]\tTime  0.048 ( 0.051)\tLoss 3.6252e+01 (3.9115e+01)\tAcc@1  79.69 ( 78.91)\tAcc@5 100.00 ( 98.66)\n",
            "Epoch: [4][300/625]\tTime  0.051 ( 0.050)\tLoss 4.4037e+01 (3.8616e+01)\tAcc@1  78.12 ( 79.01)\tAcc@5  98.44 ( 98.76)\n",
            "Epoch: [4][400/625]\tTime  0.046 ( 0.050)\tLoss 2.6853e+01 (3.8634e+01)\tAcc@1  87.50 ( 79.06)\tAcc@5  98.44 ( 98.76)\n",
            "Epoch: [4][500/625]\tTime  0.051 ( 0.050)\tLoss 3.5386e+01 (3.8523e+01)\tAcc@1  79.69 ( 79.09)\tAcc@5  98.44 ( 98.76)\n",
            "Epoch: [4][600/625]\tTime  0.050 ( 0.050)\tLoss 4.0108e+01 (3.7909e+01)\tAcc@1  79.69 ( 79.41)\tAcc@5  98.44 ( 98.80)\n",
            "==> Train Accuracy: Acc@1 79.470 || Acc@5 98.820\n",
            "==> Valid Accuracy:  Acc@1 76.580 || Acc@5 98.000\n",
            "Best Top-1 Accuracy for fold5: 76.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test with ensemble**"
      ],
      "metadata": {
        "id": "ORvLl9gcmZjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          pin_memory=True,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "qTKAvAeNmbwI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensemble\n",
        "with torch.no_grad():\n",
        "    output = None\n",
        "    predictions_list = []\n",
        "   \n",
        "    for fold in range(fold_num):\n",
        "        print(f'[Fold {fold+1}]')\n",
        "\n",
        "        if output is not None:\n",
        "            del output   # del output : memory free\n",
        "        else :\n",
        "            pass\n",
        "        path2weights_fold = f'{path2weights}_fold{fold+1}.pth'\n",
        "        model = resnet34().cuda()\n",
        "        model.load_state_dict(torch.load(path2weights_fold))\n",
        "\n",
        "        output = test(test_loader, model)\n",
        "        output = output.view(output.shape[0], output.shape[1], 1).cpu() # 각 fold별 output의 평균을 내기 위해 3번째 dimension 추가\n",
        "        predictions_list.append(output)\n",
        "\n",
        "    predictions_array = np.concatenate(predictions_list, axis=2)\n",
        "    predictions_mean = predictions_array.mean(axis = 2)\n",
        "    predictions_mean = torch.from_numpy(predictions_mean)\n",
        "    predictData = torch.argmax(predictions_mean, 1)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    lst = []\n",
        "    for Id, predict in enumerate(predictData, 1):\n",
        "        # tensor형태의 predict를 정수의 prediction으로 바꿈\n",
        "        x = predict.to(\"cpu\").numpy()\n",
        "        x = x.astype(int)\n",
        "        Category = classes[x]\n",
        "        lst.append([Id, Category])\n",
        "\n",
        "    df = DataFrame(lst, columns=['Id', 'Category'])\n",
        "    df.to_csv(path2submission, index=False, encoding='cp949')"
      ],
      "metadata": {
        "id": "KS3wl8-vdMkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7781b429-c3b3-4fe4-ee08-fa088b877312"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 33.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 35.00it/s]\n"
          ]
        }
      ]
    }
  ]
}